---
title: Introduction to Postgres indexes
date: "2025-08-19T10:00:00.000Z"
description: A developer's guide to truly understanding database indexes, and how to weild them
---

## Introduction

This post is part 1 of a 2-part series on database indexes. Part 1 is a practical, hands-on, applicable approach to indexes. We'll cover what B Trees are, of course, but with a focus on deeply understanding, and internalizing how they store data on disk, and how your database uses them to speed up queries.

This will set us up nicely for part 2, where we'll explore some interesting, counterintuitive ways to press indexes into service to achieve great querying performance over large amounts of data.

Everything in these posts will use Postgres, but absolutely everything is directly applicable to other relational databases. All the queries I'll be running are against a simple books database which I scaffolded, and had Cursor populate with about 90 million records. DDL for the database, as well as the code to fill it are in [this repo](https://github.com/arackaf/postgres-indexing-post) if you'd like to follow along on your own: `sql/db_create.sql` has the DDL, and `npx tsx insert-data/fill-database.ts` will run the code to fill it.

We'll be looking at some B Tree visualizations as we go. Those were put together with a web app I had Cursor help me build. The repo for that is [here](https://github.com/arackaf/btree-visualizer).

## Setting some baselines

Just for fun, let's take a look at the first 10 rows in the books table. Don't look too close, again, this was all algorithmically generated by AI. The special characters at the beginning were my crude way forcing the (extremely repetative) titles to spread out.

![Our data](/postgres-indexing-1/img1-data.png)

That's the last time we'll be looking at actual data. From here forward, we'll look at queries, the execution plans they generate, then we'll talk about how indexes might, or might not be able to help. Rather than the psql terminal utility I'll be running everything through DataGrip, which is an IDE for databases. The output is identical, except with nicely numbered lines, which will make things easier to talk about as we go.

Let's get started. Let's see what the prior query looks like by putting `explain analyze` before it. This tells Postgres to execute the query, and return to use the execution plan it used, as well as its performance.

```sql
explain analyze
select * from books limit 10;
```

![Execution plan](/postgres-indexing-1/img2-ex-plan.png)

We asked for 10 rows. The database did a Sequential scan on our books table, but with a limit of 10 applied. Couldn't be simpler, and returned in less than half of one millisecond. This is hardly surprising (or interesting). Postgres essentially just reached in and grabbed the first ten rows it found.

Let's grab the first 10 books, but this time sorted alphabetically.

![Sorted](/postgres-indexing-1/img3-ex-plan-sorted.png)

Catastrophically, this took 20 _seconds_. With 90 million rows in this table, Postgres now has to (kind of) sort the entire table, in order to know what the first 10 books are. I say kind of, since it doesn't _really_ have to sort the _entire_ table, just scan the entire table and grab the 10 rows with the lowest title values. That's why we see two child workers getting spawned (in addition to the main worker running out query) to each scan about a third of the table, and each take the top 7; this is reflected in lines 3-9 of the execution plan.

Line 5 makes this especially clear

```bash
->  Sort  (cost=2839564.34..2934237.14 rows=37869122 width=112) (actual time=20080.358..20080.359 rows=7 loops=3)
```

Notice the loops=3, the the rows = 37 million. Each worker is scanning its share of the table, and keeping the top 7 it sees.

These 3 groups of 7 are then _gathered_ and _merged_ together in the Gather Merge on line 2

```bash
->  Gather Merge  (cost=2840564.36..11677309.78 rows=75738244 width=112) (actual time=20093.790..20096.619 rows=10 loops=1)
```

Rather than just slapping an index in, and magically watching the time drop down, let's take a quick detour and make sure we really understand _how_ indexes work. Failing to do this can result in frustration when your database winds up not picking the index you want it to, for reasons that a good understanding could make clear.

## Indexes

The best way to think about a database index is in terms of an index in a book. These list all the major terms in the book, as well as all the pages that term appears on. Imagine you have a 1,000 page book on the American Civil War, and wanted to know what pages Philip Sheridan is mentioned on. It would be excrutiatingly slow to just look through all 1,000 pages, searching for those words. But if there's a 30 or so page index, your task is considerably simpler.

Before we go further, let's look at a very basic index over a numeric `id` column

![Index](/postgres-indexing-1/img4-index.png)

Start at the very, very bottom. Those blue "leaf" nodes contain the actual data in your index. These are the actual id values. This is a direct analogue to a book's index. So what are the gold boxes above them? These help you find _where_ the leaf node is, with the value you're looking for.

Let's go to the very top, to the root node of our B Tree. Each of these internal nodes will have N key values, and N+1 pointers. If the value you're looking for is strictly less than the first value, go down that first, left-most arrow and continue your search. If the value you're looking for is greater than or equal to that first key, but strictly less than the next key, take the second arrow. And so on. In real life the number of keys in each of these nodes will be determined by how many of them can fit into a single page on disk.

So with this B Tree, if we want to find id = 33, we start at the root. 33 is not < 19, so we don't take the first arrow. But 33 _is_ >=19 and <37, so we take the middle arrow.

Now we repeat. 33 is not < 25, so we don't take the left most path. 33 is not >= 25 **AND** < 31, so we don't take the middle path. But 33 **is** greater than 31 (it better be, this is the last path remaining), so we take the right most path. And that takes us to the leaf node with our key value.

![Index Seek](/postgres-indexing-1/btree-seek.gif)

Notice also that these leaf nodes have points forward, and backward. This allows us to not only find a _specific_ key value, but also a _range_ of values. If we wanted all ids > 33, we could do as we did, and just keep reading.

![Index Seek and Read](/postgres-indexing-1/btree-seek-read.gif)

But, now what? What if we ran a query of `SELECT * FROM books WHERE id = 33` - we've arrived at a leaf node in our index with ... our key. How do we get all the _data_ associated with that key? In other words the actual _row_ in the database for that value?

The thing I've left off so far is that leaf nodes also contain pointers to the actual table where that value in question is.

![Index](/postgres-indexing-1/btree-with-heap.png)

So the full story to find a single row in our database by an id value, via an index, would actually look more like this

![Index Seek and Read](/postgres-indexing-1/index-seek-with-heap-read.gif)

We'll talk later about these heap reads, or lack thereof when we get into covering indexes and the Index Only Scan operation.

### B Trees run in O(LogN) time

You may have been taught a fun little math fact in school, that if you were to be given a penny on January 1st, then have your penny doubled on January 2nd, then have that new amount (2 cents) doubled on January 3rd, etc, you'd have about $10 million dollars before Febuary. That's the power of exponential operations. Anytime you're repeatedly multiplying a value by some constant (which is all doubling is, for constant 2), it will become enormous, fast.

Now think of a more depressing, reverse scenrario. If someone gave you $10 million on January 1st, but with the condition that your remaining money would be halved each day, you'd have a lowly cent remaining on Feb 1st. This is a logarithm; it's the inverse of exponentiation. Rather than _multiplying_ a value by some constant, we _divide_ it by some constant. No matter how enormous, it will become small, fast.

This is exactly how B Trees work. In our example B Tree above, there were 9 leaf pages. Our internal nodes had up 3 pointers. Notice that we were able to find out the exact leaf node we wanted by reading only 2 of those gold nodes (which is also the _depth_ of the tree).

9 divided by 3 is 3
3 divided by 3 is 1

Or, more succinctly, Log<sub>3</sub>9 = 2 (the `Logarithm base 3 of 9 is 2`)

But these small values don't really do this concept justice. Imagine if you had an index with whose leaves spanned 4 bilion pages, and your index nodes had only 2 pointers, each (both of these assumptions are unrealistic). You'd _still_ need only 32 page reads to find any specific value.

2<sup>32</sup> = ~4 billion,

and also

Log<sub>2</sub>(~4 billion) = 32.

They're literally inverse operations of each other.

### How deep are real indexes?

Before we move on, let's briefly look at how deep a real Postgres index is on a somewhat large amount of data. The books table with 90 million entries already has an index defined on the primary key id field, which is a 32 bit integerer. Without going into gross detail about what all is stored on a B Tree node (N keys, N+1 offsets to other nodes, some metadata and headers, etc), ChatGPT estimates that Postgres can store between 400-500 key fields on an index on a 32 bit integer.

Let's check that.

There's a Postgres extension for just this purpose

```
CREATE EXTENSION IF NOT EXISTS pageinspect;
```

and then

```sql
SELECT * FROM bt_metap('books_pkey');
```

which produces

```
 magic  | version |  root  | level | fastroot | fastlevel | last_cleanup_num_delpages | last_cleanup_num_tuples | allequalimage
--------+---------+--------+-------+----------+-----------+---------------------------+-------------------------+---------------
 340322 |       4 | 116816 |     3 |   116816 |         3 |                         0 |                      -1 | t
```

Note the level 3, which is what our index's depth is. That means it would take just 3 index reads to arrive at the correct B Tree leaf for any value.

Checking the math, the Log<sub>450</sub>(90,000,000) comes out to ... 2.998

## Taking an index for a spin

Let's run a quick query by id, with the primary key index that already exists, and then look at how we can create on one title, so we can re-run our query to find the first 10 books in order.

```sql
explain analyze
select *
from books
where id = 10000;
```

which produces the following

![Index Seek and Read](/postgres-indexing-1/img5-index-1.png)

We're running an index scan. No surprises there. The Index Cond

```
  Index Cond: (id = 10000)
```

is the condition Postgres uses to nagivate the internal nodes; those were the gold nodes from the visualization before. In this case, it predictably looks for id = 10000

## Re-visiting our titles sort

Let's take a fresh look at this query

```sql
select *
from books
order by title
limit 10;
```

but this time let's define an index, like so

```sql
CREATE INDEX idx_title ON books(title);
```

And now our query runs in less than a ms.

![Index Seek and Read](/postgres-indexing-1/img6-index-2.png)

Notice what's _missing_ from this execution plan, that was present on the previous query, when we looked for a single index value.

Did you spot it?

It's the Index Cond. We're not actually ... _looking for_ anything. We just want the first ten rows, sorted by title. The index stores all books, sorted by title. So the engine just hops right down to the start of the index, and simply reads the first ten rows.

## More fun with indexes

Let's go deeper. Before we start, I'll point out that values for `pages` was filled with random values from 100-700. So there are 600 possible values for pages, each randomly assigned.

Let's look at a query to read the titles of books with the 3 maximum values for pages. And let's pull a lot more results this time; we'll limit it to one hundred thousand entries

```sql
explain analyze
select title, pages
from books
where pages > 697
limit 100000;
```

![Index Seek and Read](/postgres-indexing-1/img6a-top-3-page-values.png)

As before, we see a parallel sequential scan. We read through the table, looking for the first 100,000 rows. Our condition matches very few results, so the database has to discard through over 6 million records before it finds the first 100,000 matching our condition

```
   Rows Removed by Filter: 6627303
```

Let's define an index on `pages`

```sql
CREATE INDEX idx_pages ON books(pages);
```

You might notice that the pages column is by no means unique; but that doesn't effect anything: the leaf pages can easily contain duplicate entries.

![Pages index](/postgres-indexing-1/img7-pages-index.png)

Everything else works the same as it always has: the database can quickly jump down to a specific value. This allows us to query a particular value, or even grab a range of values sorted on the column we just looked up. For example, if we want all books with pages > 500, we just seek to that value, and start reading.

Let's re-run that query from before

```sql
explain analyze
select title, pages
from books
where pages > 697
limit 100000;
```

![Pages > 697](/postgres-indexing-1/img9-index-pages-bitmap-scan.png)

There's a lot going on. Our index is being used, but not like before

```bash
   ->  Bitmap Index Scan on idx_pages  (cost=0.00..4911.16 rows=451013 width=0) (actual time=38.057..38.057 rows=453891 loops=1)
```

Bitmap scan means that the database is scanning our database, and _noting_ the heap locations with records matching our filter. Remember, we need to access the heap to retrieve our title column, to satisfy our query.

Then the db scans the whole heap, and pulls out _those_ addresses. This is the Bitmap Heap Scan on line 5

```bash
   ->  Parallel Bitmap Heap Scan on books  (cost=5023.92..1441887.67 rows=187922 width=73) (actual time=41.383..1339.997 rows=33382 loops=3)
```

But remember, this is the heap, and it's not ordered on pages, so those random locations may have _other_ records _not_ matching our filter. This is done in the Index Recheck on line 6

```bash
   Recheck Cond: (pages > 697)
```

which removed 697 results.

Why is Postgres doing this, rather than just walking our index, and following the pointers to the heap, as before?

Postgres keeps track of statistics on which values are contained in its various columns. In this case, it knew that _relatively_ few values would match on this filter, so it chose to use this index.

But that still doesn't answer why it didn't use a regular old index scan, following the various pointers to the heap. Here, Postgres decided that, even though the filter would exclude a large percentage of the table, it would need to read a _lot_ of pages from the heap, and following all those random pointers from the index to the heap would be bad. Those pointers point in all manner of random directions, and **Random I/O** is bad. Instead, Postgres thought it would be better to scan the whole heap, and only reading those pages the index determined would be useful. This would potentially enable it to fetch neighboring chunks of memory from the heap, rather than frequently following those random pointers from the index.

Again, Random I/O tends to be **expensive** and can hurt query performance. This was not faulty reasoning at all.

Unfortunately in this case, Postgres wagered wrong. Our query now runs in that this was _slower_ than the regular table scan from before, on the same query. It now takes 1.38 seconds, instead of 833ms. Adding an index made this query run _slower_.

Was I forcing the issue with the larger limit of 100,000? Of course. My goal is to clearly how indexes work, how they can help, and occasionally, how they can lead the query optimizer to make the wrong choice, and hurt performance. Please don't think an index causing a worse, slower execution plan is an unhinged, unheard of eventuality which I contrived for this post; I've seen it happen on very normal queries on production databases.

Before we move on, let's query all the books with an above-average number of pages

```sql
explain analyze
select title, pages
from books
where pages > 400
limit 100000;
```

![Pages does not use index](/postgres-indexing-1/img10-pages-back-to-heap-scan.png)

In this case Postgres was smart enough to not even bother with the index.

Postgres's statistics told it that this query would match an enormous number of reasons, and just walking across the heap would get it the right results more quickly than bothering with the index. And in this case it assumed correctly. The query ran in just 37ms.

## Covering Indexes

Let's go back to this query

```sql
explain analyze
select title, pages
from books
where pages > 697
limit 100000;
```

It took a little over 800ms with no index, and over 1.3 seconds with an index on just pages.

Your first instinct might be to just add title to the index

```sql
CREATE INDEX idx_pages_title ON books(pages, title);
```

Which would look like this

![Pages and title index](/postgres-indexing-1/img11-pages-title.png)

This would work fine. We're not _needing_ to filter based on title, only pages. But having those titles in those gold non-leaf nodes wouldn't hurt one bit. Postgres would just ignore it, and find the starting point for all books with > 400 pages, and start reading. There's be no need for heap access at all, since the titles are right there.

Let's try it.

![Pages and title index](/postgres-indexing-1/img12-top-3-page-values-fast.png)

Our query now runs in just 32ms! And we have a new operation in our execution plan.

```bash
   ->  Index Only Scan using idx_pages_title on books  (cost=0.69..30393.42 rows=451013 width=73) (actual time=0.243..83.911 rows=453891 loops=1)
```

Index Only Scan means ... that _only_ the index is being scanned. Again, there's no need to look anything up in the heap, since the index has all that we need. That makes this a "covering index" for this query, since the index can "cover" it all.

More or less. Line 4

```bash
   Heap Fetches: 0
```

is not as redundant as it might seem. Postgres _does_ have to consult something called a visibility table to make sure the values in your index are up to date given how Postgres handles updates through it's MVCC system. But unless your data are changing extremely frequently this should not be a large burden.

## A variation on the theme

If you're using Postgres or Microsoft SQL Server you can create an even nicer version of this index. Remember, we're not actually querying on title here, at all. We just put it in the index so the title values would be in the leaf nodes, so Postgres could read them, without having to visit the heap.

Wouldn't it be nice it we could _only_ put those titles in the leaf nodes? This would keep our internal nodes smaller, with less content, which, in a real index, would let us cram more key values together, resulting in a smaller, shallower B Tree that would potentially be faster to query.

We do this with the INCLUDE clause when creating our index.

```sql
CREATE INDEX idx_pages_include_title ON books(pages) INCLUDE(title);
```

This tell Postgres to create our index on the pages column, as before, but also _include_ the title field in the leaf nodes. It would look like this, conceptually.

![Pages with included title index](/postgres-indexing-1/img12-index-with-included.png)

And re-running that same query, we see that it does run a bit faster. From 32ms down to just 21ms.

![Pages with included title index](/postgres-indexing-1/img13-query-with-included-col.png)

To be clear, it's quite fast either way, but a nice 31% speedup isn't something to turn down if you're using a database that supports this feature (MySQL does not).
